{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1675 items across ~84 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 84/84 [01:58<00:00,  1.41s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed. Output saved in 'leasinghub_office_buildings.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Base URL and headers (try to mimic the browser request)\n",
    "base_url = \"https://www.leasinghub.com/office/buildings\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json, text/plain, */*\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\"\n",
    "}\n",
    "\n",
    "# API parameters (query string form)\n",
    "params = {\n",
    "    \"task\": \"buildings.fetch\",\n",
    "    \"format\": \"json\",\n",
    "    \"with_images\": 1,\n",
    "    \"with_prices\": 1,\n",
    "    \"limit\": 20,\n",
    "    \"filter_order\": \"default\",\n",
    "    \"filter_order_Dir\": \"ASC\",\n",
    "    \"usage\": 1,\n",
    "    \"is_new\": \"\",\n",
    "    \"year\": \"\",\n",
    "    \"keyword\": \"\",\n",
    "    \"starting\": \"\"\n",
    "}\n",
    "\n",
    "# Output CSV file path\n",
    "output_file = \"leasinghub_office_buildings.csv\"\n",
    "\n",
    "# Write CSV headers (customize fields as needed)\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\n",
    "        \"ID\", \"Name\", \"Street Name\", \"Area Name\", \"Year Built\", \n",
    "        \"Highest Floor\", \"Typical Floor Area\", \"District Name\", \n",
    "        \"Latitude\", \"Longitude\", \"Grade\",\n",
    "        \"Min Rent\", \"Max Rent\", \"Min Price\", \"Max Price\"\n",
    "    ])\n",
    "\n",
    "max_retries = 3\n",
    "\n",
    "# --- Step 1: Get initial page to determine total pages ---\n",
    "params[\"limitstart\"] = 0\n",
    "try:\n",
    "    response_init = requests.post(base_url, headers=headers, params=params)\n",
    "except Exception as e:\n",
    "    print(f\"Initial request failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "content_type_init = response_init.headers.get(\"Content-Type\", \"\")\n",
    "if not content_type_init.startswith(\"application/json\"):\n",
    "    print(f\"Initial response is not JSON. Content-Type: {content_type_init}\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    data_init = response_init.json()\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON decode error on initial request: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Extract pager information to set up pagination\n",
    "pager = data_init.get(\"data\", {}).get(\"pager\", {})\n",
    "pages_total = pager.get(\"pagesTotal\")\n",
    "if not pages_total:\n",
    "    print(\"Could not determine total pages from the initial response.\")\n",
    "    exit(1)\n",
    "\n",
    "# Optionally, print a summary of the overall data\n",
    "total_items = pager.get(\"total\", \"unknown\")\n",
    "print(f\"Found {total_items} items across ~{pages_total} pages.\")\n",
    "\n",
    "# --- Step 2: Loop through each page with tqdm ---\n",
    "for page in tqdm(range(pages_total), desc=\"Scraping pages\", unit=\"page\"):\n",
    "    current_limitstart = page * params[\"limit\"]\n",
    "    params[\"limitstart\"] = current_limitstart\n",
    "    retries = 0\n",
    "    data = None\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.post(base_url, headers=headers, params=params)\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "            if content_type.startswith(\"application/json\"):\n",
    "                data = response.json()\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Non-JSON response at limitstart {current_limitstart}. Content-Type: {content_type}\")\n",
    "                break  # Break out—even if empty—that page will be skipped.\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error at limitstart {current_limitstart}: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(2)  # Delay before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed at limitstart {current_limitstart}: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(2)\n",
    "\n",
    "    if data is None or retries == max_retries:\n",
    "        print(f\"Skipping page with limitstart {current_limitstart} after {max_retries} retries.\")\n",
    "        continue\n",
    "\n",
    "    items = data.get(\"data\", {}).get(\"items\", [])\n",
    "    if not items:\n",
    "        # Sometimes an empty items list indicates no more data\n",
    "        # You can choose to break or continue to next page\n",
    "        print(f\"No items returned at limitstart {current_limitstart}.\")\n",
    "        continue\n",
    "\n",
    "    # Append items from this page to the CSV\n",
    "    with open(output_file, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for item in items:\n",
    "            writer.writerow([\n",
    "                item.get(\"id\"),\n",
    "                item.get(\"name_locale\"),\n",
    "                item.get(\"street_name\"),\n",
    "                item.get(\"area_name\"),\n",
    "                item.get(\"year\"),\n",
    "                item.get(\"highest_floor\"),\n",
    "                item.get(\"typfloor_area_text\"),\n",
    "                item.get(\"district_name\"),\n",
    "                item.get(\"building_lat\"),\n",
    "                item.get(\"building_lng\"),\n",
    "                item.get(\"grade\"),\n",
    "                item.get(\"stock_rents\", {}).get(\"min_rent\"),\n",
    "                item.get(\"stock_rents\", {}).get(\"max_rent\"),\n",
    "                item.get(\"stock_prices\", {}).get(\"min_price\"),\n",
    "                item.get(\"stock_prices\", {}).get(\"max_price\")\n",
    "            ])\n",
    "\n",
    "    # Optional short delay to avoid overwhelming the server\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Data scraping completed. Output saved in '{output_file}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
