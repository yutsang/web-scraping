{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current version: 133.0.6943.127\n",
    "#sudo rm /usr/local/bin/chromedriver\n",
    "#download chromedriver: npx @puppeteer/browsers install chrome@133.0.6943.127\n",
    "#or most updated channel: npx @puppeteer/browsers install chrome@stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing area: Hong Lok Yuen with URL: https://hk.centanet.com/findproperty/en/list/transaction/hong-lok-yuen_19-HMA180?q=8k0g1csup5\n",
      "\n",
      "Scraping page 1 for area: Hong Lok Yuen ...\n",
      "Found 24 rows on this page.\n",
      "Finished area: Hong Lok Yuen after 1 pages; rows found: 0\n",
      "No data extracted for area: Hong Lok Yuen\n",
      "\n",
      "Processing area: Tai Po Luxury/Mid-Levels with URL: https://hk.centanet.com/findproperty/en/list/transaction/tai-po-luxury-mid-levels_19-HMA999?q=dql2qhofgd\n",
      "\n",
      "Scraping page 1 for area: Tai Po Luxury/Mid-Levels ...\n",
      "Found 24 rows on this page.\n",
      "Extracted: 2025-02-20 | Redland Garden・Block 4・Lower Floor・Flat A | $23,000 | L | 780ft² | @$29 | Centaline\n",
      "Extracted: 2025-02-15 | The Regent・Tower 17・Lower Floor・Flat H | $19,000 | L | 598ft² | @$32 | Centaline\n",
      "\n",
      "Scraping page 2 for area: Tai Po Luxury/Mid-Levels ...\n",
      "Found 24 rows on this page.\n",
      "Finished area: Tai Po Luxury/Mid-Levels after 2 pages; rows found: 2\n",
      "Data for area 'Tai Po Luxury/Mid-Levels' saved (appended to 2025-02-21_centanet_res.csv).\n",
      "\n",
      "Processing area: Tai Po Town Centre with URL: https://hk.centanet.com/findproperty/en/list/transaction/tai-po-town-centre_19-HMA185?q=hlpk7kpqap\n",
      "\n",
      "Scraping page 1 for area: Tai Po Town Centre ...\n",
      "Found 24 rows on this page.\n",
      "Extracted: 2025-02-20 | Tai Po Plaza・Yee Fai Court (Block 2)・Upper Floor・Flat F | $11,000 | L | 377ft² | @$29 | Centaline\n",
      "Extracted: 2025-02-20 | Tai Po Centre・Phase 4・Block 7・Upper Floor・Flat H | $15,400 | L | 451ft² | @$34 | Centaline\n",
      "Extracted: 2025-02-18 | Ting Nga Court・Nga Kwan House (Block C)・4/F・Flat 6 | $4.5M | S | -- | -- | Land Registry\n",
      "Extracted: 2025-02-17 | King Nga Court・King Yuet House (Block B)・Lower Floor・Flat 6 | $5.38M | S | 554ft² | @$9,711 | Centaline\n",
      "Extracted: 2025-02-17 | Fu Heng Estate・Heng Wing House・18/F・Flat 4 | $1.78M | S | 489ft² | @$3,640 | Land Registry\n",
      "Extracted: 2025-02-17 | Treasure Garden・Block C・6/F・Flat 1 | $4.08M | S | 400ft² | @$10,200 | Land Registry\n",
      "Extracted: 2025-02-17 | King Nga Court・King Yuet House (Block B)・20/F・Flat 4 | $4.96M | S | 645ft² | @$7,690 | Land Registry\n",
      "Extracted: 2025-02-17 | Tai Wo Estate・Fook Wo House (Block 11)・6/F・Flat 19 | $2M | S | 443ft² | @$4,515 | Land Registry\n",
      "Extracted: 2025-02-17 | Serenity Park・Phase 1・Block 2・15/F・Flat G | $3.95M | S | 355ft² | @$11,127 | Land Registry\n",
      "Extracted: 2025-02-15 | Eightland Gardens・Block 2・Upper Floor・Flat E | $13,800 | L | 431ft² | @$32 | Centaline\n",
      "Extracted: 2025-02-15 | Mont Vert・Phase 1・Tower 9・Upper Floor・Flat D | $17,500 | L | 751ft² | @$23 | Centaline\n",
      "Extracted: 2025-02-15 | Tai Po Centre・Phase 1・Block 3・Upper Floor・Flat G | $12,800 | L | 361ft² | @$35 | Centaline\n",
      "Extracted: 2025-02-15 | Mont Vert・Phase 1・Tower 9・Middle Floor・Flat C | $20,000 | L | 850ft² | @$24 | Centaline\n",
      "Extracted: 2025-02-15 | Jade Plaza・Block D・Middle Floor・Flat 4 | $10,500 | L | 274ft² | @$38 | Centaline\n",
      "\n",
      "Scraping page 2 for area: Tai Po Town Centre ...\n",
      "Found 24 rows on this page.\n",
      "Finished area: Tai Po Town Centre after 2 pages; rows found: 14\n",
      "Data for area 'Tai Po Town Centre' saved (appended to 2025-02-21_centanet_res.csv).\n",
      "Scraping complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "# -------------------------\n",
    "# Utility Functions\n",
    "# -------------------------\n",
    "def generate_session_id(length=10):\n",
    "    \"\"\"Generate a random session ID consisting of lowercase letters and digits.\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "def clean_subdistrict(subdistrict):\n",
    "    \"\"\"\n",
    "    Clean the subdistrict string to generate a URL-friendly slug.\n",
    "    Any sequence of non-alphanumeric characters is replaced by a hyphen.\n",
    "    The result is lowercased and stripped of extra hyphens.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9]+', '-', subdistrict)\n",
    "    return cleaned.strip('-').lower()\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes ChromeDriver with custom options including headless mode.\"\"\"\n",
    "    chromedriver_autoinstaller.install()  # Automatically installs the correct ChromeDriver version\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.127 Safari/537.36\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--headless\")  # Enable headless mode\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def scroll_down(driver, delay=5):\n",
    "    \"\"\"Scrolls down to the bottom of the page and waits for lazy-loaded content.\"\"\"\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "def extract_data(driver, control_date_dt):\n",
    "    \"\"\"\n",
    "    Extracts listing data from the current page.\n",
    "    For each row, returns:\n",
    "      [Date, Address, Price, PriceTag, Area, Ft_Price, Agency]\n",
    "    Only rows with a date newer or equal to control_date_dt are kept.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        tbody = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.bx--structured-list-tbody\"))\n",
    "        )\n",
    "        rows = tbody.find_elements(By.CSS_SELECTOR, \"div.cv-structured-list-item\")\n",
    "        print(f\"Found {len(rows)} rows on this page.\")\n",
    "        for row in rows:\n",
    "            try:\n",
    "                cells = row.find_elements(By.CSS_SELECTOR, \"div.cv-structured-list-data\")\n",
    "                if len(cells) >= 8:\n",
    "                    # Extract Date from cell 0.\n",
    "                    date_text = cells[0].find_element(By.CSS_SELECTOR, \"div.info-date span\").text.strip()\n",
    "                    # Extract Address from cell 1.\n",
    "                    address = cells[1].text.strip()\n",
    "                    # Extract Price from cell 3 and assign PriceTag (\"S\" for tranPrice, \"L\" for tranRent).\n",
    "                    try:\n",
    "                        price_text = cells[3].find_element(By.CSS_SELECTOR, \"span.tranPrice\").text.strip()\n",
    "                        price_tag = \"S\"\n",
    "                    except Exception:\n",
    "                        price_text = cells[3].find_element(By.CSS_SELECTOR, \"span.tranRent\").text.strip()\n",
    "                        price_tag = \"L\"\n",
    "                    # Extract Saleable Area from cell 5.\n",
    "                    area = cells[5].text.strip()\n",
    "                    # Extract Unit Price from cell 6.\n",
    "                    ft_price = cells[6].text.strip()\n",
    "                    # Extract Agency from cell 7 (try label01 then label).\n",
    "                    try:\n",
    "                        agency = cells[7].find_element(By.CSS_SELECTOR, \"span.label01\").text.strip()\n",
    "                    except Exception:\n",
    "                        agency = cells[7].find_element(By.CSS_SELECTOR, \"span.label\").text.strip()\n",
    "                    \n",
    "                    # Parse the row date and skip if older than control date.\n",
    "                    try:\n",
    "                        row_date_dt = datetime.strptime(date_text, \"%Y-%m-%d\")\n",
    "                    except Exception:\n",
    "                        row_date_dt = None\n",
    "                    if row_date_dt and row_date_dt < control_date_dt:\n",
    "                        continue\n",
    "\n",
    "                    data.append([date_text, address, price_text, price_tag, area, ft_price, agency])\n",
    "                    print(f\"Extracted: {date_text} | {address} | {price_text} | {price_tag} | {area} | {ft_price} | {agency}\")\n",
    "            except Exception as row_err:\n",
    "                print(\"Error extracting a row:\", row_err)\n",
    "    except Exception as e:\n",
    "        print(\"Error locating transaction table body:\", e)\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    # Base URL for the live site.\n",
    "    base_url = \"https://hk.centanet.com/findproperty/en/list/transaction\"\n",
    "    \n",
    "    # Set the control date (YYYY-MM-DD); adjust this value as needed.\n",
    "    control_date = \"2025-01-03\"\n",
    "    control_date_dt = datetime.strptime(control_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Read the area codes file (ensure the file is UTF-8 friendly).\n",
    "    try:\n",
    "        area_df = pd.read_excel(\"Centanet_Res_Area_Code.xlsx\", engine=\"openpyxl\")\n",
    "    except Exception as e:\n",
    "        print(\"Error reading Centanet_Res_Area_Code.xlsx:\", e)\n",
    "        return\n",
    "\n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    # Incremental saving: define the output file.\n",
    "    file_path = f\"{datetime.today().strftime('%Y-%m-%d')}_centanet_res.csv\"\n",
    "    # Remove existing file if exists to start fresh.\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Process each row of the area codes file.\n",
    "        for idx, row in area_df.iterrows():\n",
    "            region = row[\"Region\"]\n",
    "            district = row[\"District\"]\n",
    "            subdistrict = row[\"Subdistrict\"]\n",
    "            code = row[\"Code\"]\n",
    "\n",
    "            # Clean the subdistrict string for URL formation.\n",
    "            subdistrict_part = clean_subdistrict(subdistrict)\n",
    "            session_id = generate_session_id()  # Generate a new session id.\n",
    "            area_url = f\"{base_url}/{subdistrict_part}_19-{code}?q={session_id}\"\n",
    "            print(f\"\\nProcessing area: {subdistrict} with URL: {area_url}\")\n",
    "\n",
    "            driver.get(area_url)\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.bx--structured-list-tbody\"))\n",
    "            )\n",
    "            \n",
    "            area_rows = []\n",
    "            current_page = 1\n",
    "\n",
    "            while True:\n",
    "                print(f\"\\nScraping page {current_page} for area: {subdistrict} ...\")\n",
    "                scroll_down(driver, delay=5)\n",
    "                page_data = extract_data(driver, control_date_dt)\n",
    "                for d in page_data:\n",
    "                    # Append area info along with the constructed area URL.\n",
    "                    area_rows.append(d + [region, district, subdistrict, code, area_url])\n",
    "                if not page_data:  # No new data on this page; assume done.\n",
    "                    break\n",
    "                try:\n",
    "                    next_button = WebDriverWait(driver, 10).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.btn-next:not([disabled])\"))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    time.sleep(5)\n",
    "                    current_page += 1\n",
    "                except Exception as e:\n",
    "                    print(\"No more pages or next page button not clickable; moving to next area:\", e)\n",
    "                    break\n",
    "            \n",
    "            print(f\"Finished area: {subdistrict} after {current_page} pages; rows found: {len(area_rows)}\")\n",
    "            if area_rows:\n",
    "                df = pd.DataFrame(area_rows,\n",
    "                                  columns=[\"Date\", \"Address\", \"Price\", \"PriceTag\", \"Area\", \"Ft_Price\", \"Agency\",\n",
    "                                           \"Region\", \"District\", \"Subdistrict\", \"Code\", \"Area_URL\"])\n",
    "                # Incremental save: append to CSV file.\n",
    "                df.to_csv(file_path, mode=\"a\", index=False, header=not os.path.exists(file_path), encoding=\"utf-8-sig\")\n",
    "                print(f\"Data for area '{subdistrict}' saved (appended to {file_path}).\")\n",
    "            else:\n",
    "                print(f\"No data extracted for area: {subdistrict}\")\n",
    "            driver.delete_all_cookies()\n",
    "            time.sleep(3)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Scraping complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
