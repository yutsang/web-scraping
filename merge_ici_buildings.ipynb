{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 353 (3588616495.py, line 356)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 356\u001b[0;36m\u001b[0m\n\u001b[0;31m    for idx, row in centanet_df.iterrows():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 353\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import csv\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def normalize_building_name(name: str) -> str:\n",
    "    \"\"\"Normalize building names to facilitate matching.\"\"\"\n",
    "    \n",
    "    \"\"\"Normalize building names to facilitate matching.\"\"\"\n",
    "    if name is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Ensure name is a string\n",
    "    try:\n",
    "        name = str(name)\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    \n",
    "    # Replace common abbreviations with standardized forms\n",
    "    replacements = {\n",
    "        'bldg': 'building', \n",
    "        'ctr': 'center', \n",
    "        'centre': 'center',\n",
    "        'plz': 'plaza', \n",
    "        'twr': 'tower', \n",
    "        'indl': 'industrial',\n",
    "        'comm': 'commercial', \n",
    "        'ltd': 'limited', \n",
    "        'hse': 'house',\n",
    "        'intl': 'international',\n",
    "        'dev': 'development',\n",
    "        'fty': 'factory',\n",
    "        'gdn': 'garden',\n",
    "        'rd': 'road',\n",
    "        'st': 'street',\n",
    "        'ave': 'avenue'\n",
    "    }\n",
    "    \n",
    "    # Replace abbreviations with full words\n",
    "    for old, new in replacements.items():\n",
    "        name = re.sub(r'\\b' + old + r'\\b', new, name)\n",
    "    \n",
    "    # Remove punctuation and normalize whitespace\n",
    "    name = re.sub(r'[^\\w\\s]', ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    \n",
    "    return name\n",
    "\n",
    "def normalize_address(address: str) -> str:\n",
    "    \"\"\"Normalize addresses to facilitate matching.\"\"\"\n",
    "    if address is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Ensure address is a string\n",
    "    try:\n",
    "        address = str(address)\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    address = address.lower()\n",
    "    \n",
    "    # Standardize road/street/avenue abbreviations\n",
    "    replacements = {\n",
    "        'rd': 'road',\n",
    "        'rd.': 'road',\n",
    "        'st': 'street',\n",
    "        'st.': 'street',\n",
    "        'ave': 'avenue',\n",
    "        'ave.': 'avenue'\n",
    "    }\n",
    "    \n",
    "    # Replace abbreviations with full words\n",
    "    for old, new in replacements.items():\n",
    "        address = re.sub(r'\\b' + old + r'\\b', new, address)\n",
    "    \n",
    "    # Normalize special characters and whitespace\n",
    "    address = re.sub(r'[^\\w\\s]', ' ', address)\n",
    "    address = re.sub(r'\\s+', ' ', address).strip()\n",
    "    \n",
    "    return address\n",
    "\n",
    "def extract_year(year_string: str) -> Optional[int]:\n",
    "    \"\"\"Extract year from various string formats.\"\"\"\n",
    "    if not isinstance(year_string, str):\n",
    "        return None\n",
    "    \n",
    "    # Try to find a 4-digit year\n",
    "    year_match = re.search(r'(\\d{4})', year_string)\n",
    "    if year_match:\n",
    "        return int(year_match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_csv_with_fallbacks(file_path, **kwargs):\n",
    "    \"\"\"Load CSV with multiple fallback methods if errors occur.\"\"\"\n",
    "    try:\n",
    "        # First attempt - standard loading\n",
    "        return pd.read_csv(file_path, **kwargs)\n",
    "    except pd.errors.ParserError as e:\n",
    "        logger.warning(f\"Parser error with {file_path}: {str(e)}, trying with on_bad_lines='skip'\")\n",
    "        try:\n",
    "            # Second attempt - skip bad lines\n",
    "            return pd.read_csv(file_path, on_bad_lines='skip', **kwargs)\n",
    "        except Exception as e2:\n",
    "            logger.warning(f\"Failed with on_bad_lines: {str(e2)}, trying with different quoting\")\n",
    "            try:\n",
    "                # Third attempt - adjust quoting\n",
    "                return pd.read_csv(file_path, quoting=csv.QUOTE_NONE, escapechar='\\\\', **kwargs)\n",
    "            except Exception as e3:\n",
    "                logger.warning(f\"All parsing methods failed: {str(e3)}, trying with engine='python'\")\n",
    "                try:\n",
    "                    # Fourth attempt - use Python engine\n",
    "                    return pd.read_csv(file_path, engine='python', **kwargs)\n",
    "                except Exception as e4:\n",
    "                    logger.error(f\"All CSV parsing methods failed: {str(e4)}\")\n",
    "                    # Create an empty DataFrame with the same structure\n",
    "                    return pd.DataFrame()\n",
    "\n",
    "def preprocess_csv_file(input_path, output_path=None):\n",
    "    \"\"\"Manually preprocess a problematic CSV file to fix inconsistent columns.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = input_path + \".fixed.csv\"\n",
    "    \n",
    "    # Read the file to determine the expected number of columns\n",
    "    with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        expected_columns = first_line.count(',') + 1\n",
    "        logger.info(f\"Expected columns in CSV: {expected_columns}\")\n",
    "    \n",
    "    # Process the file line by line\n",
    "    with open(input_path, 'r', encoding='utf-8', errors='ignore') as infile, \\\n",
    "         open(output_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        problem_lines = 0\n",
    "        \n",
    "        for i, line in enumerate(infile, 1):\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            # Split the line by commas not enclosed in quotes\n",
    "            fields = []\n",
    "            in_quotes = False\n",
    "            current_field = ''\n",
    "            \n",
    "            for char in line:\n",
    "                if char == '\"':\n",
    "                    in_quotes = not in_quotes\n",
    "                    current_field += char\n",
    "                elif char == ',' and not in_quotes:\n",
    "                    fields.append(current_field)\n",
    "                    current_field = ''\n",
    "                else:\n",
    "                    current_field += char\n",
    "            \n",
    "            # Add the last field\n",
    "            fields.append(current_field)\n",
    "            \n",
    "            # Check if we have the right number of columns\n",
    "            if len(fields) != expected_columns:\n",
    "                problem_lines += 1\n",
    "                logger.warning(f\"Line {i}: Found {len(fields)} fields, expected {expected_columns}\")\n",
    "                \n",
    "                if len(fields) > expected_columns:\n",
    "                    # Combine extra fields into the last expected field\n",
    "                    combined = fields[:expected_columns-1]\n",
    "                    combined.append(','.join(fields[expected_columns-1:]))\n",
    "                    fields = combined\n",
    "                else:\n",
    "                    # Pad with empty fields if fewer columns\n",
    "                    fields.extend([''] * (expected_columns - len(fields)))\n",
    "            \n",
    "            writer.writerow(fields)\n",
    "    \n",
    "    logger.info(f\"Preprocessing complete. Fixed {problem_lines} problem lines. Output saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def calculate_similarity(building1: Dict, building2: Dict) -> float:\n",
    "    \"\"\"Calculate similarity score between two buildings with improved type handling.\"\"\"\n",
    "    # Calculate name similarity using token sort ratio (handles word order differences)\n",
    "    name_similarity = fuzz.token_sort_ratio(\n",
    "        str(building1.get('normalized_name', '')), \n",
    "        str(building2.get('normalized_name', ''))\n",
    "    )\n",
    "    \n",
    "    # Calculate name similarity using token set ratio (handles partial matches)\n",
    "    name_set_similarity = fuzz.token_set_ratio(\n",
    "        str(building1.get('normalized_name', '')), \n",
    "        str(building2.get('normalized_name', ''))\n",
    "    )\n",
    "    \n",
    "    # Get the best name similarity score\n",
    "    best_name_similarity = max(name_similarity, name_set_similarity)\n",
    "    \n",
    "    # Compare address if available\n",
    "    address_similarity = 0\n",
    "    if building1.get('address') is not None and building2.get('address') is not None:\n",
    "        address1 = normalize_address(str(building1.get('address', '')))\n",
    "        address2 = normalize_address(str(building2.get('address', '')))\n",
    "        address_similarity = fuzz.token_sort_ratio(address1, address2)\n",
    "    \n",
    "    # Check district match\n",
    "    district_match = 0\n",
    "    if building1.get('district') and building2.get('district'):\n",
    "        # Convert to string to ensure type consistency\n",
    "        if str(building1.get('district', '')).lower() == str(building2.get('district', '')).lower():\n",
    "            district_match = 100\n",
    "    \n",
    "    # Extract address numbers and check for matches\n",
    "    address_number_match = 0\n",
    "    try:\n",
    "        addr1_nums = re.findall(r'\\d+', str(building1.get('address', '')))\n",
    "        addr2_nums = re.findall(r'\\d+', str(building2.get('address', '')))\n",
    "        if addr1_nums and addr2_nums:\n",
    "            common_numbers = set(addr1_nums) & set(addr2_nums)\n",
    "            if common_numbers:\n",
    "                address_number_match = 100\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting address numbers: {str(e)}\")\n",
    "        address_number_match = 0\n",
    "    \n",
    "    # Calculate weighted score\n",
    "    weights = {\n",
    "        'name': 0.6,\n",
    "        'address': 0.2,\n",
    "        'district': 0.1,\n",
    "        'address_number': 0.1\n",
    "    }\n",
    "    \n",
    "    weighted_score = (\n",
    "        best_name_similarity * weights['name'] +\n",
    "        address_similarity * weights['address'] +\n",
    "        district_match * weights['district'] +\n",
    "        address_number_match * weights['address_number']\n",
    "    )\n",
    "    \n",
    "    return weighted_score\n",
    "\n",
    "\n",
    "def create_consolidated_database(leasinghub_df=None, centanet_df=None, midland_df=None, match_threshold=75):\n",
    "    \"\"\"Create a consolidated database of buildings from multiple sources.\"\"\"\n",
    "    consolidated = {}\n",
    "    \n",
    "    # Process LeasingHub data first (assumes most complete source with grade info)\n",
    "    if leasinghub_df is not None and not leasinghub_df.empty:\n",
    "        logger.info(f\"Processing {len(leasinghub_df)} buildings from LeasingHub\")\n",
    "        \n",
    "        # Normalize LeasingHub data\n",
    "        for idx, row in leasinghub_df.iterrows():\n",
    "            key = f\"LH_{row.get('ID', idx)}\"\n",
    "            \n",
    "            year_built = None\n",
    "            if pd.notna(row.get('Year Built')):\n",
    "                try:\n",
    "                    year_built = int(row.get('Year Built'))\n",
    "                except (ValueError, TypeError):\n",
    "                    year_built = extract_year(str(row.get('Year Built')))\n",
    "            \n",
    "            consolidated[key] = {\n",
    "                'source': 'leasinghub',\n",
    "                'source_id': row.get('ID'),\n",
    "                'original_key': row.get('ID'),  # Keep original key for verification\n",
    "                'building_name': row.get('Name', \"\"),\n",
    "                'normalized_name': normalize_building_name(row.get('Name', \"\")),\n",
    "                'address': row.get('Street Name', \"\"),\n",
    "                'district': row.get('District Name', \"\"),\n",
    "                'grade': row.get('Grade'),\n",
    "                'year_built': year_built,\n",
    "                'original_record': row.to_dict(),  # Store original record for eyeball check\n",
    "                'matches': {}  # Will store matches to other sources\n",
    "            }\n",
    "    \n",
    "    # Process Midland data\n",
    "    if midland_df is not None and not midland_df.empty:\n",
    "        logger.info(f\"Processing {len(midland_df)} buildings from Midland\")\n",
    "        \n",
    "        # Store unmatched records for later addition\n",
    "        unmatched_midland = []\n",
    "        \n",
    "        for idx, row in midland_df.iterrows():\n",
    "            # Extract building ID from the URL if available\n",
    "            building_id = None\n",
    "            if 'Detail URL' in row:\n",
    "                url_match = re.search(r'B\\d+', row.get('Detail URL', ''))\n",
    "                if url_match:\n",
    "                    building_id = url_match.group(0)\n",
    "            \n",
    "            midland_data = {\n",
    "                'source': 'midland',\n",
    "                'source_id': building_id,\n",
    "                'original_key': building_id,  # Keep original key for verification\n",
    "                'building_name': row.get('Building Name', \"\"),\n",
    "                'normalized_name': normalize_building_name(row.get('Building Name', \"\")),\n",
    "                'address': row.get('Address', \"\"),\n",
    "                'original_record': row.to_dict(),  # Store original record\n",
    "                'matches': {}\n",
    "            }\n",
    "            \n",
    "            # Try to match to existing buildings from LeasingHub\n",
    "            matched = False\n",
    "            best_match_key = None\n",
    "            best_match_score = 0\n",
    "            \n",
    "            for cons_key, cons_data in consolidated.items():\n",
    "                match_score = calculate_similarity(midland_data, cons_data)\n",
    "                if match_score >= match_threshold and match_score > best_match_score:\n",
    "                    best_match_key = cons_key\n",
    "                    best_match_score = match_score\n",
    "            \n",
    "            if best_match_key:\n",
    "                # Add as a match to existing record\n",
    "                consolidated[best_match_key]['matches']['midland'] = {\n",
    "                    'id': midland_data['source_id'],\n",
    "                    'original_key': building_id,  # Store original key\n",
    "                    'score': best_match_score,\n",
    "                    'building_name': midland_data['building_name'],\n",
    "                    'address': midland_data['address'],\n",
    "                    'original_record': row.to_dict()  # Add original record to match info\n",
    "                }\n",
    "                matched = True\n",
    "            \n",
    "            if not matched:\n",
    "                unmatched_midland.append(midland_data)\n",
    "        \n",
    "        # Add unmatched Midland buildings as new records\n",
    "        logger.info(f\"Adding {len(unmatched_midland)} unmatched Midland buildings\")\n",
    "        for data in unmatched_midland:\n",
    "            key = f\"MD_{data['source_id'] if data['source_id'] else str(hash(data['building_name']))}\"\n",
    "            consolidated[key] = data\n",
    "            consolidated[key]['matches'] = {}  # Initialize empty matches dict\n",
    "    \n",
    "    # Process Centanet data and match to existing records\n",
    "    if centanet_df is not None and not centanet_df.empty:\n",
    "        logger.info(f\"Processing {len(centanet_df)} buildings from Centanet\")\n",
    "        \n",
    "        # Store unmatched records for later addition\n",
    "        unmatched_centanet = []\n",
    "        \n",
    "        for idx, row in centanet_df.iterrows():\n",
    "            \n",
    "            try:\n",
    "                # Extract data safely with type checking\n",
    "                property_id = str(row.get('propertyID')) if 'propertyID' in row else None\n",
    "                building_name = str(row.get('buildingNameEn')) if 'buildingNameEn' in row and row.get('buildingNameEn') is not None else None\n",
    "                address = str(row.get('address')) if 'address' in row and row.get('address') is not None else None\n",
    "                district = str(row.get('districtNameEn')) if 'districtNameEn' in row and row.get('districtNameEn') is not None else None\n",
    "                \n",
    "                if not building_name:  # Skip records without a building name\n",
    "                    continue\n",
    "                    \n",
    "                centanet_data = {\n",
    "                    'source': 'centanet',\n",
    "                    'source_id': property_id,\n",
    "                    'original_key': property_id,\n",
    "                    'building_name': building_name,\n",
    "                    'normalized_name': normalize_building_name(building_name),\n",
    "                    'address': address,\n",
    "                    'district': district,\n",
    "                    'original_record': row.to_dict() if hasattr(row, 'to_dict') else {},\n",
    "                    'matches': {}\n",
    "                }\n",
    "                centanet_data = ensure_string_fields(centanet_data)\n",
    "                \n",
    "                # Rest of matching logic...\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing Centanet building at index {idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # Try to match to existing buildings\n",
    "            matched = False\n",
    "            best_match_key = None\n",
    "            best_match_score = 0\n",
    "            \n",
    "            for cons_key, cons_data in consolidated.items():\n",
    "                match_score = calculate_similarity(centanet_data, cons_data)\n",
    "                if match_score >= match_threshold and match_score > best_match_score:\n",
    "                    best_match_key = cons_key\n",
    "                    best_match_score = match_score\n",
    "            \n",
    "            if best_match_key:\n",
    "                # Add as a match to existing record\n",
    "                consolidated[best_match_key]['matches']['centanet'] = {\n",
    "                    'id': centanet_data['source_id'],\n",
    "                    'original_key': property_id,  # Store original key\n",
    "                    'score': best_match_score,\n",
    "                    'building_name': centanet_data['building_name'],\n",
    "                    'address': centanet_data['address'],\n",
    "                    'original_record': row.to_dict()  # Add original record to match info\n",
    "                }\n",
    "                matched = True\n",
    "            \n",
    "            if not matched:\n",
    "                unmatched_centanet.append(centanet_data)\n",
    "        \n",
    "        # Add unmatched Centanet buildings as new records\n",
    "        logger.info(f\"Adding {len(unmatched_centanet)} unmatched Centanet buildings\")\n",
    "        for data in unmatched_centanet:\n",
    "            key = f\"CN_{data['source_id'] if data['source_id'] else str(hash(data['building_name']))}\"\n",
    "            consolidated[key] = data\n",
    "            consolidated[key]['matches'] = {}  # Initialize empty matches dict\n",
    "    \n",
    "    logger.info(f\"Consolidated database contains {len(consolidated)} buildings\")\n",
    "    return consolidated\n",
    "\n",
    "def save_consolidated_database(building_db, output_file=\"consolidated_buildings.json\"):\n",
    "    \"\"\"Save the consolidated database to both JSON and CSV formats.\"\"\"\n",
    "    if not building_db:\n",
    "        logger.warning(\"Empty building database, nothing to save\")\n",
    "        return\n",
    "        \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    records = []\n",
    "    for key, building in building_db.items():\n",
    "        building_copy = building.copy()\n",
    "        building_copy['building_key'] = key\n",
    "        \n",
    "        # Convert matches to a string representation for CSV output\n",
    "        if 'matches' in building_copy:\n",
    "            building_copy['matches_json'] = json.dumps(building_copy['matches'])\n",
    "            # Extract matched IDs for easier analysis\n",
    "            midland_id = building_copy.get('matches', {}).get('midland', {}).get('id', '')\n",
    "            centanet_id = building_copy.get('matches', {}).get('centanet', {}).get('id', '')\n",
    "            building_copy['midland_id'] = midland_id\n",
    "            building_copy['centanet_id'] = centanet_id\n",
    "            \n",
    "            # Extract original keys for verification\n",
    "            midland_orig_key = building_copy.get('matches', {}).get('midland', {}).get('original_key', '')\n",
    "            centanet_orig_key = building_copy.get('matches', {}).get('centanet', {}).get('original_key', '')\n",
    "            building_copy['midland_original_key'] = midland_orig_key\n",
    "            building_copy['centanet_original_key'] = centanet_orig_key\n",
    "        \n",
    "        # Remove large objects for CSV output\n",
    "        if 'original_record' in building_copy:\n",
    "            del building_copy['original_record']\n",
    "        if 'matches' in building_copy:\n",
    "            del building_copy['matches']\n",
    "        \n",
    "        records.append(building_copy)\n",
    "    \n",
    "    db_df = pd.DataFrame(records)\n",
    "    \n",
    "    # Save as JSON for preserving nested structures\n",
    "    if output_file.endswith('.json'):\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Also save as CSV for easy viewing\n",
    "    csv_file = output_file.replace('.json', '.csv')\n",
    "    db_df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    logger.info(f\"Saved {len(records)} buildings to {output_file} and {csv_file}\")\n",
    "    return db_df\n",
    "\n",
    "def generate_mapping_tables(building_db):\n",
    "    \"\"\"Generate mapping tables between different sources for easier reference.\"\"\"\n",
    "    mappings = {\n",
    "        'leasinghub_to_midland': [],\n",
    "        'leasinghub_to_centanet': [],\n",
    "        'midland_to_centanet': []\n",
    "    }\n",
    "    \n",
    "    for key, building in building_db.items():\n",
    "        source = building.get('source')\n",
    "        source_id = building.get('source_id')\n",
    "        original_key = building.get('original_key')\n",
    "        \n",
    "        if source == 'leasinghub':\n",
    "            # Map LeasingHub to Midland\n",
    "            if 'midland' in building.get('matches', {}):\n",
    "                midland_match = building['matches']['midland']\n",
    "                mappings['leasinghub_to_midland'].append({\n",
    "                    'building_key': key,\n",
    "                    'leasinghub_id': source_id,\n",
    "                    'leasinghub_original_key': original_key,\n",
    "                    'leasinghub_name': building.get('building_name'),\n",
    "                    'midland_id': midland_match.get('id'),\n",
    "                    'midland_original_key': midland_match.get('original_key'),\n",
    "                    'midland_name': midland_match.get('building_name'),\n",
    "                    'match_score': midland_match.get('score')\n",
    "                })\n",
    "            \n",
    "            # Map LeasingHub to Centanet\n",
    "            if 'centanet' in building.get('matches', {}):\n",
    "                centanet_match = building['matches']['centanet']\n",
    "                mappings['leasinghub_to_centanet'].append({\n",
    "                    'building_key': key,\n",
    "                    'leasinghub_id': source_id,\n",
    "                    'leasinghub_original_key': original_key,\n",
    "                    'leasinghub_name': building.get('building_name'),\n",
    "                    'centanet_id': centanet_match.get('id'),\n",
    "                    'centanet_original_key': centanet_match.get('original_key'),\n",
    "                    'centanet_name': centanet_match.get('building_name'),\n",
    "                    'match_score': centanet_match.get('score')\n",
    "                })\n",
    "        \n",
    "        elif source == 'midland':\n",
    "            # Map Midland to Centanet\n",
    "            if 'centanet' in building.get('matches', {}):\n",
    "                centanet_match = building['matches']['centanet']\n",
    "                mappings['midland_to_centanet'].append({\n",
    "                    'building_key': key,\n",
    "                    'midland_id': source_id,\n",
    "                    'midland_original_key': original_key,\n",
    "                    'midland_name': building.get('building_name'),\n",
    "                    'centanet_id': centanet_match.get('id'),\n",
    "                    'centanet_original_key': centanet_match.get('original_key'),\n",
    "                    'centanet_name': centanet_match.get('building_name'),\n",
    "                    'match_score': centanet_match.get('score')\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrames and save\n",
    "    for mapping_name, mapping_data in mappings.items():\n",
    "        if mapping_data:\n",
    "            df = pd.DataFrame(mapping_data)\n",
    "            df.to_csv(f\"{mapping_name}_mapping.csv\", index=False)\n",
    "            logger.info(f\"Saved {len(df)} records to {mapping_name}_mapping.csv\")\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "def ensure_string_fields(data_dict: Dict) -> Dict:\n",
    "    \"\"\"Ensure all relevant fields are strings.\"\"\"\n",
    "    string_fields = ['building_name', 'normalized_name', 'address', 'district']\n",
    "    for field in string_fields:\n",
    "        if field in data_dict:\n",
    "            if data_dict[field] is None:\n",
    "                data_dict[field] = \"\"\n",
    "            else:\n",
    "                data_dict[field] = str(data_dict[field])\n",
    "    return data_dict\n",
    "\n",
    "def generate_eyeball_check_report(building_db, output_file='eyeball_check_report.csv'):\n",
    "    \"\"\"Generate a report for eyeball checking of building matches.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for key, building in building_db.items():\n",
    "        source = building.get('source')\n",
    "        base_record = {\n",
    "            'building_key': key,\n",
    "            'source': source,\n",
    "            'source_id': building.get('source_id'),\n",
    "            'original_key': building.get('original_key'),\n",
    "            'building_name': building.get('building_name'),\n",
    "            'address': building.get('address'),\n",
    "            'district': building.get('district', ''),\n",
    "            'grade': building.get('grade', ''),\n",
    "            'year_built': building.get('year_built', '')\n",
    "        }\n",
    "        \n",
    "        # If no matches, just add this record\n",
    "        if not building.get('matches'):\n",
    "            records.append(base_record)\n",
    "            continue\n",
    "        \n",
    "        # For each match, create a record that shows both buildings\n",
    "        for match_source, match_info in building.get('matches', {}).items():\n",
    "            match_record = base_record.copy()\n",
    "            match_record['match_source'] = match_source\n",
    "            match_record['match_id'] = match_info.get('id')\n",
    "            match_record['match_original_key'] = match_info.get('original_key')\n",
    "            match_record['match_name'] = match_info.get('building_name')\n",
    "            match_record['match_address'] = match_info.get('address', '')\n",
    "            match_record['match_score'] = match_info.get('score')\n",
    "            records.append(match_record)\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"Eyeball check report saved to {output_file}\")\n",
    "    return df\n",
    "\n",
    "def load_source_data():\n",
    "    \"\"\"Load data from various sources with robust error handling.\"\"\"\n",
    "    leasinghub_df = None\n",
    "    centanet_df = None\n",
    "    midland_df = None\n",
    "    \n",
    "    # Load LeasingHub data if available\n",
    "    if os.path.exists('leasinghub_office_buildings.csv'):\n",
    "        logger.info(\"Loading LeasingHub data\")\n",
    "        leasinghub_df = load_csv_with_fallbacks('leasinghub_office_buildings.csv')\n",
    "        if not leasinghub_df.empty:\n",
    "            logger.info(f\"Loaded {len(leasinghub_df)} buildings from LeasingHub\")\n",
    "    \n",
    "    # Load Midland data if available\n",
    "    if os.path.exists('midlandici_building_list.csv'):\n",
    "        logger.info(\"Loading Midland data\")\n",
    "        midland_df = load_csv_with_fallbacks('midlandici_building_list.csv')\n",
    "        if not midland_df.empty:\n",
    "            logger.info(f\"Loaded {len(midland_df)} buildings from Midland\")\n",
    "    \n",
    "    # Load Centanet data with robust error handling\n",
    "    if os.path.exists('centanet_ici_buildings.csv'):\n",
    "        logger.info(\"Loading Centanet data\")\n",
    "        try:\n",
    "            # First try the fallback loading methods\n",
    "            centanet_df = load_csv_with_fallbacks('centanet_ici_buildings.csv')\n",
    "            \n",
    "            if centanet_df.empty:\n",
    "                logger.warning(\"Automatic loading methods failed, preprocessing the CSV file...\")\n",
    "                # If all automatic methods fail, try manual preprocessing\n",
    "                fixed_csv = preprocess_csv_file('centanet_ici_buildings.csv')\n",
    "                centanet_df = pd.read_csv(fixed_csv)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(centanet_df)} buildings from Centanet\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load Centanet data: {str(e)}\")\n",
    "    \n",
    "    return leasinghub_df, centanet_df, midland_df\n",
    "\n",
    "def process_transaction_data(building_db):\n",
    "    \"\"\"Process and enrich transaction data with building information.\"\"\"\n",
    "    # Process Centanet transactions\n",
    "    if os.path.exists('centanet_transactions.csv'):\n",
    "        logger.info(\"Processing Centanet transaction data\")\n",
    "        centanet_trans = load_csv_with_fallbacks('centanet_transactions.csv')\n",
    "        \n",
    "        if not centanet_trans.empty:\n",
    "            enriched_trans = enrich_transaction_data(\n",
    "                centanet_trans, \n",
    "                building_db, \n",
    "                source='centanet',\n",
    "                name_col='buildingNameEn',\n",
    "                id_col='propertyID'\n",
    "            )\n",
    "            \n",
    "            enriched_trans.to_csv('enriched_centanet_transactions.csv', index=False)\n",
    "            logger.info(f\"Saved enriched Centanet transactions to enriched_centanet_transactions.csv\")\n",
    "    \n",
    "    # Process Midland transactions\n",
    "    if os.path.exists('midland_transactions.csv'):\n",
    "        logger.info(\"Processing Midland transaction data\")\n",
    "        midland_trans = load_csv_with_fallbacks('midland_transactions.csv')\n",
    "        \n",
    "        if not midland_trans.empty:\n",
    "            enriched_trans = enrich_transaction_data(\n",
    "                midland_trans, \n",
    "                building_db, \n",
    "                source='midland',\n",
    "                name_col='building_name',\n",
    "                id_col='building_id'\n",
    "            )\n",
    "            \n",
    "            enriched_trans.to_csv('enriched_midland_transactions.csv', index=False)\n",
    "            logger.info(f\"Saved enriched Midland transactions to enriched_midland_transactions.csv\")\n",
    "\n",
    "def enrich_transaction_data(transactions_df, building_db, source='midland', name_col='building_name', id_col=None):\n",
    "    \"\"\"Enrich transaction data with building information from consolidated database.\"\"\"\n",
    "    # Add new columns for the enriched data\n",
    "    transactions_df['building_key'] = None\n",
    "    transactions_df['original_building_id'] = None  # Store original ID for verification\n",
    "    transactions_df['building_grade'] = None\n",
    "    transactions_df['building_year'] = None\n",
    "    transactions_df['match_confidence'] = None\n",
    "    \n",
    "    # Process each transaction\n",
    "    for idx, row in transactions_df.iterrows():\n",
    "        building_name = row.get(name_col) if name_col in row else None\n",
    "        building_id = row.get(id_col) if id_col and id_col in row else None\n",
    "        \n",
    "        if not building_name and not building_id:\n",
    "            continue\n",
    "        \n",
    "        # Try direct match by source ID first\n",
    "        if building_id:\n",
    "            # Search for direct match\n",
    "            for key, building in building_db.items():\n",
    "                # Check if this is a direct match\n",
    "                if building['source'] == source and building.get('source_id') == building_id:\n",
    "                    transactions_df.loc[idx, 'building_key'] = key\n",
    "                    transactions_df.loc[idx, 'original_building_id'] = building.get('original_key')\n",
    "                    transactions_df.loc[idx, 'building_grade'] = building.get('grade')\n",
    "                    transactions_df.loc[idx, 'building_year'] = building.get('year_built')\n",
    "                    transactions_df.loc[idx, 'match_confidence'] = 100\n",
    "                    break\n",
    "                \n",
    "                # Check in matches\n",
    "                if source in building.get('matches', {}):\n",
    "                    match_info = building['matches'][source]\n",
    "                    if match_info.get('id') == building_id:\n",
    "                        transactions_df.loc[idx, 'building_key'] = key\n",
    "                        transactions_df.loc[idx, 'original_building_id'] = match_info.get('original_key')\n",
    "                        transactions_df.loc[idx, 'building_grade'] = building.get('grade')\n",
    "                        transactions_df.loc[idx, 'building_year'] = building.get('year_built')\n",
    "                        transactions_df.loc[idx, 'match_confidence'] = match_info.get('score')\n",
    "                        break\n",
    "        \n",
    "        # If no match by ID, try fuzzy matching on name\n",
    "        if pd.isna(transactions_df.loc[idx, 'building_key']) and building_name:\n",
    "            norm_name = normalize_building_name(building_name)\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "            \n",
    "            for key, building in building_db.items():\n",
    "                # Calculate multiple similarity metrics\n",
    "                name_score = fuzz.token_sort_ratio(norm_name, building.get('normalized_name', ''))\n",
    "                name_set_score = fuzz.token_set_ratio(norm_name, building.get('normalized_name', ''))\n",
    "                \n",
    "                best_name_score = max(name_score, name_set_score)\n",
    "                \n",
    "                # Check if this is better than previous matches and meets threshold\n",
    "                if best_name_score > best_score and best_name_score >= 85:\n",
    "                    best_match = key\n",
    "                    best_score = best_name_score\n",
    "            \n",
    "            if best_match:\n",
    "                transactions_df.loc[idx, 'building_key'] = best_match\n",
    "                \n",
    "                # Store original key for the appropriate source\n",
    "                if building_db[best_match]['source'] == source:\n",
    "                    transactions_df.loc[idx, 'original_building_id'] = building_db[best_match].get('original_key')\n",
    "                elif source in building_db[best_match].get('matches', {}):\n",
    "                    transactions_df.loc[idx, 'original_building_id'] = building_db[best_match]['matches'][source].get('original_key')\n",
    "                \n",
    "                transactions_df.loc[idx, 'building_grade'] = building_db[best_match].get('grade')\n",
    "                transactions_df.loc[idx, 'building_year'] = building_db[best_match].get('year_built')\n",
    "                transactions_df.loc[idx, 'match_confidence'] = best_score\n",
    "    \n",
    "    return transactions_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for the real estate data consolidation application.\"\"\"\n",
    "    logging.info(\"Starting real estate data consolidation process\")\n",
    "    \n",
    "    # Step 1: Load data from various sources\n",
    "    leasinghub_df, centanet_df, midland_df = load_source_data()\n",
    "    \n",
    "    # Step 2: Create consolidated building database\n",
    "    building_db = create_consolidated_database(\n",
    "        leasinghub_df=leasinghub_df,\n",
    "        centanet_df=centanet_df,\n",
    "        midland_df=midland_df,\n",
    "        match_threshold=75  # Configurable threshold for building matching\n",
    "    )\n",
    "    \n",
    "    # Step 3: Save consolidated database and generate mapping tables\n",
    "    db_df = save_consolidated_database(building_db, 'consolidated_buildings.json')\n",
    "    mappings = generate_mapping_tables(building_db)\n",
    "    eyeball_df = generate_eyeball_check_report(building_db, 'eyeball_check_report.csv')\n",
    "    \n",
    "    # Step 4: Process transaction data (if available)\n",
    "    process_transaction_data(building_db)\n",
    "    \n",
    "    logging.info(\"Real estate data consolidation process completed successfully\")\n",
    "    \n",
    "    # Return the database and reports for further analysis in the notebook\n",
    "    return building_db, db_df, mappings\n",
    "\n",
    "# For Jupyter notebook execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        building_db, db_df, mappings = main()\n",
    "        print(f\"Successfully consolidated data for {len(building_db)} buildings\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main process: {str(e)}\")\n",
    "        logging.exception(\"Stack trace:\")\n",
    "        print(f\"Error occurred: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
