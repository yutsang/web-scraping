{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing areas: 100%|██████████| 178/178 [1:55:26<00:00, 38.92s/it]  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_session_id(length=10):\n",
    "    \"\"\"Generate a random session ID consisting of lowercase letters and digits.\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "def clean_subdistrict(subdistrict):\n",
    "    \"\"\"\n",
    "    Clean the subdistrict string to generate a URL-friendly slug.\n",
    "    Any sequence of non-alphanumeric characters is replaced by a hyphen.\n",
    "    The result is lowercased and stripped of extra hyphens.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9]+', '-', subdistrict)\n",
    "    return cleaned.strip('-').lower()\n",
    "\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    Initializes ChromeDriver with custom options including headless mode.\n",
    "    chromedriver_autoinstaller installs the correct version if needed.\n",
    "    \"\"\"\n",
    "    chromedriver_autoinstaller.install()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.127 Safari/537.36\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--headless\")  # Enable headless mode for background execution\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def random_sleep(min_delay=1, max_delay=3):\n",
    "    \"\"\"Pause execution for a random duration between min_delay and max_delay seconds.\"\"\"\n",
    "    time.sleep(random.uniform(min_delay, max_delay))\n",
    "\n",
    "\n",
    "def scroll_down(driver):\n",
    "    \"\"\"Scrolls down to the bottom of the page to trigger lazy-loaded content.\"\"\"\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    random_sleep()\n",
    "\n",
    "\n",
    "def extract_estate_data(driver):\n",
    "    \"\"\"\n",
    "    Extracts estate information from the current page.\n",
    "\n",
    "    The expected DOM structure is:\n",
    "      - Name and Address are inside <div class=\"flex f-dir-col basic-info\">\n",
    "      - Other details are inside <div class=\"flex basic-data hidden-xs-only\">\n",
    "            Blocks, Units, Unit Rate, MoM, Trans Record, For Sale, For Rent.\n",
    "      - Estate link is inside <a class=\"property-text flex def-property-box\"> via href attribute.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        estate_items = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.property-text.flex.def-property-box\"))\n",
    "        )\n",
    "        for item in estate_items:\n",
    "            try:\n",
    "                # Extract estate link from anchor tag\n",
    "                estate_link = item.get_attribute(\"href\")\n",
    "\n",
    "                # Extract name and address from basic-info section\n",
    "                name = item.find_element(By.CSS_SELECTOR, \"div.main-text\").text.strip()\n",
    "                address = item.find_element(By.CSS_SELECTOR, \"div.address.f-middle\").text.strip()\n",
    "\n",
    "                # Extract other details from basic-data section\n",
    "                blocks = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Block(s)')]/following-sibling::div\").text.strip()\n",
    "                units = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Units')]/following-sibling::div\").text.strip()\n",
    "                unit_rate = item.find_element(By.XPATH, \".//div[contains(text(), 'Unit Rate of Saleable Area')]/following-sibling::div\").text.strip()\n",
    "                mom = item.find_element(By.XPATH, \".//div[contains(text(), 'MoM')]/following-sibling::div\").text.strip()\n",
    "                trans_record = item.find_element(By.XPATH, \".//div[contains(text(), 'Trans. Record')]/following-sibling::div\").text.strip()\n",
    "                for_sale = item.find_element(By.XPATH, \".//div[contains(text(), 'For Sale')]/following-sibling::div\").text.strip()\n",
    "                for_rent = item.find_element(By.XPATH, \".//div[contains(text(), 'For Rent')]/following-sibling::div\").text.strip()\n",
    "\n",
    "                data.append([name, address, blocks, units, unit_rate, mom, trans_record, for_sale, for_rent, estate_link])\n",
    "            except Exception:\n",
    "                continue  # Skip item if any field fails to extract\n",
    "    except Exception:\n",
    "        pass  # Skip page if no estate items found\n",
    "    return data\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Base URL for the estate listings.\n",
    "    base_url = \"https://hk.centanet.com/findproperty/en/list/estate\"\n",
    "    \n",
    "    # Read area codes from the Excel file.\n",
    "    try:\n",
    "        area_df = pd.read_excel(\"Centanet_Res_Area_Code.xlsx\", engine=\"openpyxl\")\n",
    "    except Exception as e:\n",
    "        print(\"Error reading Centanet_Res_Area_Code.xlsx:\", e)\n",
    "        return\n",
    "\n",
    "    driver = initialize_driver()\n",
    "    file_path = f\"{datetime.today().strftime('%Y-%m-%d')}_centanet_estates.csv\"\n",
    "    \n",
    "    # Remove existing CSV file if exists.\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Iterate over each area with a progress bar.\n",
    "        for idx, row in tqdm(area_df.iterrows(), total=area_df.shape[0], desc=\"Processing areas\"):\n",
    "            region = row[\"Region\"]\n",
    "            district = row[\"District\"]\n",
    "            subdistrict = row[\"Subdistrict\"]\n",
    "            code = row[\"Code\"]\n",
    "            subdistrict_part = clean_subdistrict(subdistrict)\n",
    "            session_id = generate_session_id()\n",
    "            area_url = f\"{base_url}/{subdistrict_part}_19-{code}?q={session_id}\"\n",
    "            driver.get(area_url)\n",
    "            #random_sleep()\n",
    "\n",
    "            current_page = 1\n",
    "            area_rows = []\n",
    "            while True:\n",
    "                scroll_down(driver)\n",
    "                page_data = extract_estate_data(driver)\n",
    "                if page_data:\n",
    "                    for row_data in page_data:\n",
    "                        area_rows.append(row_data + [region, district, subdistrict, code])\n",
    "                else:\n",
    "                    break  # Exit loop if no data found on this page\n",
    "\n",
    "                try:\n",
    "                    next_button = WebDriverWait(driver, 10).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.btn-next:not([disabled])\"))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    random_sleep()\n",
    "                    current_page += 1\n",
    "                except Exception:\n",
    "                    break  # Exit loop if no next page button found\n",
    "\n",
    "            if area_rows:\n",
    "                df = pd.DataFrame(area_rows,\n",
    "                                  columns=[\"Name\", \"Address\", \"Blocks\", \"Units\", \"Unit Rate\", \"MoM\", \"Trans Record\",\n",
    "                                           \"For Sale\", \"For Rent\", \"Estate Link\", \"Region\", \"District\", \"Subdistrict\",\n",
    "                                           \"Code\"])\n",
    "                df.to_csv(file_path, mode=\"a\", index=False, header=not os.path.exists(file_path), encoding=\"utf-8-sig\")\n",
    "            driver.delete_all_cookies()\n",
    "            random_sleep()\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file found: 2025-03-07_centanet_estates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   0%|          | 2/19685 [00:20<55:11:22, 10.09s/it]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def generate_session_id(length=10):\n",
    "    \"\"\"Generate a random session ID consisting of lowercase letters and digits.\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "def clean_subdistrict(subdistrict):\n",
    "    \"\"\"\n",
    "    Clean the subdistrict string to generate a URL-friendly slug.\n",
    "    Any sequence of non-alphanumeric characters is replaced by a hyphen.\n",
    "    The result is lowercased and stripped of extra hyphens.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9]+', '-', subdistrict)\n",
    "    return cleaned.strip('-').lower()\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    Initializes ChromeDriver with custom options including headless mode.\n",
    "    chromedriver_autoinstaller installs the correct version if needed.\n",
    "    \"\"\"\n",
    "    chromedriver_autoinstaller.install()  # Automatically installs/updates chromedriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.127 Safari/537.36\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--headless\")  # Run headlessly to speed up scraping\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def random_sleep(min_delay=1, max_delay=3):\n",
    "    \"\"\"Pause execution for a random duration between min_delay and max_delay seconds.\"\"\"\n",
    "    time.sleep(random.uniform(min_delay, max_delay))\n",
    "\n",
    "def scroll_down(driver):\n",
    "    \"\"\"Scrolls down to trigger lazy-loaded content.\"\"\"\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    random_sleep()\n",
    "\n",
    "# Initialize the Selenium driver\n",
    "driver = initialize_driver()\n",
    "\n",
    "# Find all CSV files that follow the naming pattern *_centanet_estates.csv\n",
    "csv_files = glob.glob(\"*_centanet_estates.csv\")\n",
    "date_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})_centanet_estates\\.csv\")\n",
    "dated_files = [\n",
    "    (pd.to_datetime(match.group(1)), file)\n",
    "    for file in csv_files if (match := date_pattern.search(file))\n",
    "]\n",
    "\n",
    "if not dated_files:\n",
    "    print(\"No CSV files matching the specified pattern found.\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Select the CSV with the latest date in its filename\n",
    "latest_date, latest_file = max(dated_files, key=lambda x: x[0])\n",
    "print(\"Latest file found:\", latest_file)\n",
    "\n",
    "# Read the original CSV without modifying it directly.\n",
    "df = pd.read_csv(latest_file)\n",
    "\n",
    "# Create new columns for scraped data if they don't already exist.\n",
    "for col in [\"Scraped Estate Name\", \"Occupation Permit\", \"Scraped Blocks\",\n",
    "            \"Scraped Units\", \"School Net Info\", \"Estate Detailed Address\", \"Developer\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "# Define the output file path (adding a _scraped suffix)\n",
    "new_file_path = latest_file.replace(\"_centanet_estates.csv\", \"_centanet_estates_scraped.csv\")\n",
    "\n",
    "# Iterate over each row using tqdm for progress indication\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing URLs\"):\n",
    "    url = row[\"Estate Link\"]\n",
    "    #print(f\"Processing URL: {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        random_sleep(2, 3)  # Allow the page to load\n",
    "        scroll_down(driver)  # Scroll to load lazy-loaded content if needed\n",
    "\n",
    "        # Extract Estate Name\n",
    "        try:\n",
    "            estate_name_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-title\")\n",
    "            estate_name = estate_name_elem.text.strip()\n",
    "        except Exception:\n",
    "            estate_name = None\n",
    "\n",
    "        # Extract other details: Occupation Permit, Blocks, Units\n",
    "        occupation, blocks_text, units_text = None, None, None\n",
    "        try:\n",
    "            table_items = driver.find_elements(By.CLASS_NAME, \"table-item\")\n",
    "            for item in table_items:\n",
    "                try:\n",
    "                    title_elem = item.find_element(By.CLASS_NAME, \"table-item-title\")\n",
    "                    text_elem = item.find_element(By.CLASS_NAME, \"table-item-text\")\n",
    "                    text_content = text_elem.text.strip()\n",
    "                    if \"Date of Occupation Permit\" in text_content:\n",
    "                        occupation = title_elem.text.strip()\n",
    "                    elif \"No. of Blocks\" in text_content:\n",
    "                        blocks_text = title_elem.text.strip().split()[0]\n",
    "                    elif \"No. of Units\" in text_content:\n",
    "                        units_text = title_elem.text.strip()\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Extract School Net information\n",
    "        school_net_val = None\n",
    "        try:\n",
    "            items_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "            for div in items_divs:\n",
    "                try:\n",
    "                    label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                    if \"School Net\" in label_elem.text.strip():\n",
    "                        links_elems = div.find_elements(By.TAG_NAME, \"a\")\n",
    "                        if len(links_elems) >= 2:\n",
    "                            primary_net = links_elems[0].text.strip()\n",
    "                            secondary_net = links_elems[1].text.strip()\n",
    "                            school_net_val = f\"{primary_net} | {secondary_net}\"\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Extract Estate Detailed Address\n",
    "        estate_address = None\n",
    "        try:\n",
    "            address_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-position\")\n",
    "            estate_address = address_elem.text.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Extract Developer information\n",
    "        developer_val = None\n",
    "        try:\n",
    "            developer_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "            for div in developer_divs:\n",
    "                try:\n",
    "                    label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                    if \"Developer\" in label_elem.text.strip():\n",
    "                        developer_span_elem = div.find_element(By.CLASS_NAME, \"label-item-right\")\n",
    "                        developer_val = developer_span_elem.text.strip()\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Save the scraped data into the DataFrame (for the current row only)\n",
    "        df.at[idx, \"Scraped Estate Name\"] = estate_name\n",
    "        df.at[idx, \"Occupation Permit\"] = occupation\n",
    "        df.at[idx, \"Scraped Blocks\"] = blocks_text\n",
    "        df.at[idx, \"Scraped Units\"] = units_text\n",
    "        df.at[idx, \"School Net Info\"] = school_net_val\n",
    "        df.at[idx, \"Estate Detailed Address\"] = estate_address\n",
    "        df.at[idx, \"Developer\"] = developer_val\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "    \n",
    "    # Write the current DataFrame to CSV to prevent data loss after each iteration\n",
    "    df.to_csv(new_file_path, index=False)\n",
    "    \n",
    "    # Pause briefly before processing the next URL\n",
    "    random_sleep(2, 3)\n",
    "\n",
    "print(f\"Scraped data saved to: {new_file_path}\")\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
